{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATM_0NvmvJXw"
      },
      "source": [
        "# HR AI Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview\n",
        "\n",
        "In this project, I create a recruitment workflow using LangGraph, LangChain, and various APIs to automate and streamline the job posting and candidate evaluation process. The workflow consists of the following key steps:\n",
        "\n",
        "* Requirements Gathering: The AI agent prompts the user for detailed job requirements, including the job title, company description, candidate responsibilities and qualifications, preferred location, and other relevant details.\n",
        "\n",
        "* Job Description Generation: Once the job requirements are gathered, the agent generates a professional and compelling job description. The user can then review and approve the description, or provide feedback for the agent to refine it.\n",
        "\n",
        "* LinkedIn Candidate Search and Outreach: If the user provides specific LinkedIn profiles of preferred candidates, the agent will directly message them about the opportunity. Alternatively, the agent can search LinkedIn for relevant candidates based on the job details and send outreach messages.\n",
        "\n",
        "* CV Analysis: As candidates submit their CVs, the agent evaluates them against the job requirements, providing a score and recommendation (approve or reject) for each applicant. The agent then sends the appropriate response message to the candidate via LinkedIn.\n",
        "\n",
        "* Interview Question Preparation: For approved candidates, the agent generates a set of interview questions covering technical skills, experience, and problem-solving. The user can review and approve the questions before the interviews are conducted."
      ],
      "metadata": {
        "id": "RR0G4A9yV85Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation\n",
        "\n",
        "The motivation for this project is to automate and streamline the recruitment process, which can be time-consuming and labor-intensive for HR professionals. By leveraging AI and various APIs, the goal is to create an efficient and effective system that can handle tasks such as job posting, candidate outreach, CV analysis, and interview preparation, allowing HR teams to focus on other important aspects of the hiring process."
      ],
      "metadata": {
        "id": "71BPHxoDWDvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation\n",
        "\n",
        "The code is implemented using a combination of LangGraph, LangChain, and various APIs, including the Anthropic API for language modeling, the Google Serper API for LinkedIn searches, and the LinkedIn API for outreach and profile retrieval.\n",
        "\n",
        "The code is organized into several sections:\n",
        "* Importing Required Libraries: The necessary libraries and APIs are imported at the beginning of the notebook.\n",
        "* Defining Base Models: The code defines the base models for job requirements, candidate profiles, and the overall recruitment state.\n",
        "* Defining Tool Functions: The code defines the tool functions that the agent will use, such as searching for LinkedIn candidates, retrieving LinkedIn profile information, and sending LinkedIn messages.\n",
        "* Defining Workflow Nodes: The code defines the various nodes in the recruitment workflow, such as requirements gathering, job description generation, LinkedIn process, CV analysis, and interview question preparation.\n",
        "* Creating the Recruitment Workflow: The code combines the workflow nodes and sets up the overall recruitment workflow, including the use of breakpoints for human-in-the-loop interactions.\n",
        "* Running the Workflow: The code demonstrates how to run the recruitment workflow, handle interrupts, and update the state of the process.\n",
        "\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "This project demonstrates the power of AI-powered automation in the recruitment process. By leveraging LangGraph, LangChain, and various APIs, the agent is able to streamline tasks such as job posting, candidate outreach, CV analysis, and interview preparation, freeing up HR professionals to focus on other important aspects of the hiring process. The use of human-in-the-loop interactions allows for a balance between automation and human oversight, ensuring the quality and accuracy of the recruitment process."
      ],
      "metadata": {
        "id": "DKRVCaYzXZ7J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exsYcl5UvYc3"
      },
      "source": [
        "# Installing required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Js765NcYELqC",
        "outputId": "ce68c6f4-d481-4c2b-a036-d3d55c8ea436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Collecting langchain-anthropic\n",
            "  Downloading langchain_anthropic-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.53-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.9.2)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting anthropic<1,>=0.39.0 (from langchain-anthropic)\n",
            "  Downloading anthropic-0.39.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from langchain-anthropic) (0.7.1)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.5-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.32 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.36-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain)\n",
            "  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.39.0->langchain-anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.39.0->langchain-anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.39.0->langchain-anthropic) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.39.0->langchain-anthropic) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.39.0->langchain-anthropic) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.39.0->langchain-anthropic) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.39.0->langchain-anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1,>=0.39.0->langchain-anthropic) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_anthropic-0.3.0-py3-none-any.whl (22 kB)\n",
            "Downloading langgraph-0.2.53-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.1/125.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.8-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anthropic-0.39.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.4/198.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.5-py3-none-any.whl (24 kB)\n",
            "Downloading langgraph_sdk-0.1.36-py3-none-any.whl (29 kB)\n",
            "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: SQLAlchemy, python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, dataclasses-json, anthropic, langchain-core, langgraph-checkpoint, langchain-anthropic, langgraph, langchain, langchain_community\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.19\n",
            "    Uninstalling langchain-core-0.3.19:\n",
            "      Successfully uninstalled langchain-core-0.3.19\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.7\n",
            "    Uninstalling langchain-0.3.7:\n",
            "      Successfully uninstalled langchain-0.3.7\n",
            "Successfully installed SQLAlchemy-2.0.35 anthropic-0.39.0 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.8 langchain-anthropic-0.3.0 langchain-core-0.3.21 langchain_community-0.3.8 langgraph-0.2.53 langgraph-checkpoint-2.0.5 langgraph-sdk-0.1.36 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-anthropic langgraph python-dotenv pydantic langchain_community requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RcnKWPYtIiOs"
      },
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "from typing import Dict, Any, List, Optional, Literal, Annotated, TypedDict\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from datetime import datetime\n",
        "import operator\n",
        "import json\n",
        "import uuid\n",
        "import requests\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.errors import NodeInterrupt\n",
        "\n",
        "# Display imports\n",
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jjK6aljDWuef"
      },
      "outputs": [],
      "source": [
        "# API Keys\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"ANTHROPIC_API_KEY\"\n",
        "os.environ[\"SERPER_API_KEY\"] = \"SERPER_API_KEY\"\n",
        "os.environ[\"LINKEDIN_COOKIE\"] = \"LINKEDIN_COOKIE\"\n",
        "\n",
        "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0)\n",
        "\n",
        "# Initialize search utility\n",
        "search = GoogleSerperAPIWrapper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txviulRpr2-n"
      },
      "source": [
        " # Defining base models\n",
        "\n",
        " Defines models for job requirements, candidate profiles, and overall recruitment state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yDqf9YahWx3r"
      },
      "outputs": [],
      "source": [
        "# Job requirements model\n",
        "class JobRequirements(BaseModel):\n",
        "    # Details about the job requirements\n",
        "    title: str\n",
        "    company_description: str\n",
        "    job_requirements: List[str]\n",
        "    candidate_responsibilities: List[str]\n",
        "    candidate_qualifications: List[str]\n",
        "    company_benefits: List[str]\n",
        "    interview_date: datetime\n",
        "    preferred_country: str\n",
        "    years_experience: int\n",
        "    linkedin_profiles: Optional[List[str]] = None\n",
        "    skills_required: List[str]\n",
        "    salary_range: str\n",
        "\n",
        "\n",
        "\n",
        "# Candidate profile model\n",
        "class CandidateProfile(BaseModel):\n",
        "    # Details about the candidate\n",
        "    name: str\n",
        "    linkedin_url: str\n",
        "    title: str\n",
        "    location: str\n",
        "    cv_content: Optional[str] = None\n",
        "    cv_score: Optional[float] = None\n",
        "    status: str = \"new\"  # 'new', 'contacted', 'cv_received', 'approved', 'rejected'\n",
        "    feedback: Optional[str] = None\n",
        "    source: str = \"direct\"  # 'direct' or 'search'\n",
        "    skill_matches: Optional[List[str]] = None\n",
        "    skill_gaps: Optional[List[str]] = None\n",
        "    message_sent: Optional[str] = None\n",
        "\n",
        "# Main State Model\n",
        "class RecruitmentState(BaseModel):\n",
        "    # Overall state of the recruitment process\n",
        "    phase: str\n",
        "    messages: Annotated[List[BaseMessage], add_messages]\n",
        "    job_requirements: Optional[JobRequirements]\n",
        "    job_description: Optional[str]\n",
        "    job_description_approved: bool\n",
        "    candidates: Annotated[List[CandidateProfile], operator.add]\n",
        "    linkedin_process_complete: bool\n",
        "    cv_analysis_complete: bool\n",
        "    interview_questions: Optional[List[str]]\n",
        "    interview_questions_approved: bool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xM3gjh0sMPh"
      },
      "source": [
        "# Defining tool functions\n",
        "Defines functions for LinkedIn searches, profile retrieval, and messaging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gpcAMpRUW4WG"
      },
      "outputs": [],
      "source": [
        "# Search for LinkedIn candidates using Google Serper API\n",
        "@tool\n",
        "def search_linkedin_candidates(\n",
        "    job_title: str,\n",
        "    location: str,\n",
        "    skills: List[str],\n",
        "    limit: int = 5\n",
        ") -> List[Dict[str, str]]:\n",
        "    \"\"\"Search for candidates on LinkedIn using Google Search API\"\"\"\n",
        "    search = GoogleSerperAPIWrapper()\n",
        "    search_query = f\"\"\"site:linkedin.com/in/\n",
        "        {job_title}\n",
        "        {location}\n",
        "        {' '.join(skills)}\"\"\"\n",
        "\n",
        "    try:\n",
        "        results = search.results(search_query)\n",
        "        candidates = []\n",
        "        for result in results.get('organic', [])[:limit]:\n",
        "            if \"linkedin.com/in/\" in result.get(\"link\", \"\"):\n",
        "                candidates.append({\n",
        "                    \"linkedin_url\": result[\"link\"],\n",
        "                    \"title\": result.get(\"title\", \"\"),\n",
        "                    \"snippet\": result.get(\"snippet\", \"\")\n",
        "                })\n",
        "        return candidates\n",
        "    except Exception as e:\n",
        "        return [{\"error\": str(e)}]\n",
        "\n",
        "\n",
        "# Get detailed LinkedIn profile information\n",
        "@tool\n",
        "def get_linkedin_profile(profile_url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Get detailed profile information from LinkedIn URL\"\"\"\n",
        "    headers = {\n",
        "        'cookie': os.getenv(\"LINKEDIN_COOKIE\"),\n",
        "        'accept': 'application/json'\n",
        "    }\n",
        "    try:\n",
        "        profile_id = profile_url.split('/in/')[-1].split('/')[0]\n",
        "        api_url = f\"https://www.linkedin.com/voyager/api/identity/profiles/{profile_id}/profileView\"\n",
        "        response = requests.get(api_url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        return {\n",
        "            \"name\": f\"{data.get('firstName', '')} {data.get('lastName', '')}\",\n",
        "            \"title\": data.get('headline', ''),\n",
        "            \"location\": data.get('locationName', ''),\n",
        "            \"skills\": [skill.get('name', '') for skill in data.get('skills', [])]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "\n",
        "# Send a message to a LinkedIn candidate about the job opportunity\n",
        "@tool\n",
        "def send_linkedin_message(profile_url: str, job_details: str) -> Dict[str, str]:\n",
        "    \"\"\"Send a LinkedIn message to a candidate about the job opportunity\"\"\"\n",
        "    headers = {\n",
        "        'cookie': os.getenv(\"LINKEDIN_COOKIE\"),\n",
        "        'accept': 'application/json'\n",
        "    }\n",
        "    try:\n",
        "        profile_id = profile_url.split('/in/')[-1].split('/')[0]\n",
        "\n",
        "        # Generate personalized message\n",
        "        message_prompt = [\n",
        "            SystemMessage(content=\"Generate a personalized LinkedIn outreach message for a job opportunity. Ask them to submit their CV directly through LinkedIn messages if interested.\"),\n",
        "            HumanMessage(content=f\"Job Details:\\n{job_details}\")\n",
        "        ]\n",
        "        message_content = llm.invoke(message_prompt).content\n",
        "\n",
        "        # Send message via LinkedIn API\n",
        "        message_endpoint = f\"https://www.linkedin.com/voyager/api/messaging/conversations\"\n",
        "        payload = {\n",
        "            \"recipients\": [profile_id],\n",
        "            \"message\": message_content\n",
        "        }\n",
        "        response = requests.post(message_endpoint, headers=headers, json=payload)\n",
        "\n",
        "        return {\n",
        "            \"status\": \"sent\",\n",
        "            \"profile_id\": profile_id,\n",
        "            \"message\": message_content\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXa3oKh0sfp7"
      },
      "source": [
        "# Defining workflow nodes\n",
        "Defines nodes for requirements gathering, job description, CV analysis, etc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uPF4UD5vW7tx"
      },
      "outputs": [],
      "source": [
        "def requirements_gathering(state: RecruitmentState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Initialize or continue requirements gathering process.\n",
        "\n",
        "    Args:\n",
        "        state: Current recruitment workflow state\n",
        "\n",
        "    Returns:\n",
        "        Dict with state updates\n",
        "    \"\"\"\n",
        "    if not state.messages:  # Using dot notation instead of dict notation\n",
        "        # Get job requirements input using the correct JobRequirements model\n",
        "        job_requirements = JobRequirements(\n",
        "            title=input(\"What is the job title? \"),\n",
        "            company_description=input(\"Provide a description of the company: \"),\n",
        "            job_requirements=input(\"List the job requirements (comma-separated): \").split(\",\"),\n",
        "            candidate_responsibilities=input(\"List the candidate responsibilities (comma-separated): \").split(\",\"),\n",
        "            candidate_qualifications=input(\"List the candidate qualifications (comma-separated): \").split(\",\"),\n",
        "            company_benefits=input(\"List the company benefits (comma-separated): \").split(\",\"),\n",
        "            interview_date=datetime.strptime(\n",
        "                input(\"What is the interview date (YYYY-MM-DD)? \"),\n",
        "                \"%Y-%m-%d\"\n",
        "            ),\n",
        "            preferred_country=input(\"What is the preferred country for the role? \"),\n",
        "            years_experience=int(input(\"What is the required years of experience? \")),\n",
        "            linkedin_profiles=[\n",
        "                p.strip()\n",
        "                for p in input(\"Provide any LinkedIn profile URLs (comma-separated, optional): \").split(\",\")\n",
        "                if p.strip()\n",
        "            ],\n",
        "            skills_required=input(\"List the required skills (comma-separated): \").split(\",\"),\n",
        "            salary_range=input(\"What is the salary range for the role? \")\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"messages\": [\n",
        "                SystemMessage(content=\"You are an HR assistant gathering detailed job requirements.\"),\n",
        "                HumanMessage(content=\"Let's begin gathering the job requirements. What is the job title?\")\n",
        "            ],\n",
        "            \"job_requirements\": job_requirements,\n",
        "            \"phase\": \"requirements_gathering\"\n",
        "        }\n",
        "    else:\n",
        "        return {\"phase\": state.phase}  # Return current state if messages exist\n",
        "\n",
        "\n",
        "\n",
        "def generate_job_desc(state: RecruitmentState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generate job description with human-in-the-loop review.\n",
        "\n",
        "    Args:\n",
        "        state: Current recruitment workflow state\n",
        "\n",
        "    Returns:\n",
        "        Dict with state updates\n",
        "\n",
        "    Raises:\n",
        "        NodeInterrupt: To get human review of generated description\n",
        "    \"\"\"\n",
        "    if not state[\"job_requirements\"]:\n",
        "        raise ValueError(\"Job requirements missing\")\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=\"\"\"Create a professional and compelling job description with:\n",
        "        1. About the Company\n",
        "        2. Role Overview\n",
        "        3. Key Responsibilities\n",
        "        4. Required Qualifications\n",
        "        5. What We Offer (Benefits)\n",
        "        6. Location and Work Mode\"\"\"),\n",
        "        HumanMessage(content=str(state[\"job_requirements\"]))\n",
        "    ]\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    raise NodeInterrupt(\n",
        "        f\"Please review the generated job description:\\n\\n{response.content}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEAg-s34sp1z"
      },
      "source": [
        "# LinkedIn process node\n",
        "Handles LinkedIn candidate search, outreach, and messaging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hraKSYGjW--0"
      },
      "outputs": [],
      "source": [
        "def linkedin_process(state: RecruitmentState) -> Dict[str, Any]:\n",
        "     # Implementation to handle the LinkedIn candidate search and outreach process\n",
        "    \"\"\"Handle LinkedIn candidate search/outreach process\"\"\"\n",
        "    candidates = []\n",
        "    tool_node = ToolNode([search_linkedin_candidates, get_linkedin_profile, send_linkedin_message])\n",
        "\n",
        "    job_details = f\"\"\"\n",
        "    Role: {state.job_requirements.title}\n",
        "    Company: {state.job_requirements.company_description}\n",
        "    Location: {state.job_requirements.preferred_country}\n",
        "    Description: {state.job_description}\n",
        "    \"\"\"\n",
        "\n",
        "    if state.job_requirements.linkedin_profiles:\n",
        "        # Process provided profiles\n",
        "        for profile_url in state.job_requirements.linkedin_profiles:\n",
        "            profile = tool_node.invoke({\n",
        "                \"name\": \"get_linkedin_profile\",\n",
        "                \"args\": {\"profile_url\": profile_url}\n",
        "            })\n",
        "\n",
        "            if \"error\" not in profile:\n",
        "                message_result = tool_node.invoke({\n",
        "                    \"name\": \"send_linkedin_message\",\n",
        "                    \"args\": {\n",
        "                        \"profile_url\": profile_url,\n",
        "                        \"job_details\": job_details\n",
        "                    }\n",
        "                })\n",
        "\n",
        "                if \"error\" not in message_result:\n",
        "                    candidates.append(\n",
        "                        CandidateProfile(\n",
        "                            **profile,\n",
        "                            linkedin_url=profile_url,\n",
        "                            status=\"contacted\",\n",
        "                            message_sent=message_result[\"message\"]\n",
        "                        )\n",
        "                    )\n",
        "    else:\n",
        "        # Search for candidates using Serper API\n",
        "        search_results = tool_node.invoke({\n",
        "            \"name\": \"search_linkedin_candidates\",\n",
        "            \"args\": {\n",
        "                \"job_title\": state.job_requirements.title,\n",
        "                \"location\": state.job_requirements.preferred_country,\n",
        "                \"skills\": state.job_requirements.skills_required\n",
        "            }\n",
        "        })\n",
        "\n",
        "        for result in search_results:\n",
        "            if \"error\" not in result:\n",
        "                profile = tool_node.invoke({\n",
        "                    \"name\": \"get_linkedin_profile\",\n",
        "                    \"args\": {\"profile_url\": result[\"linkedin_url\"]}\n",
        "                })\n",
        "\n",
        "                if \"error\" not in profile:\n",
        "                    message_result = tool_node.invoke({\n",
        "                        \"name\": \"send_linkedin_message\",\n",
        "                        \"args\": {\n",
        "                            \"profile_url\": result[\"linkedin_url\"],\n",
        "                            \"job_details\": job_details\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "                    if \"error\" not in message_result:\n",
        "                        candidates.append(\n",
        "                            CandidateProfile(\n",
        "                                **profile,\n",
        "                                linkedin_url=result[\"linkedin_url\"],\n",
        "                                status=\"contacted\",\n",
        "                                source=\"search\",\n",
        "                                message_sent=message_result[\"message\"]\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "    return {\n",
        "        \"candidates\": candidates,\n",
        "        \"linkedin_process_complete\": True,\n",
        "        \"phase\": \"analyze_cv\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeW8KGpvs6aY"
      },
      "source": [
        "# CV analysis node\n",
        "Evaluates candidate CVs and sends approval/rejection messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C1KvOF62XBxt"
      },
      "outputs": [],
      "source": [
        "def analyze_cv(state: RecruitmentState) -> Dict[str, Any]:\n",
        "    # Implementation to analyze candidate CVs and send appropriate responses\n",
        "    \"\"\"Analyze CV and send appropriate response\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"\"\"Analyze the CV against job requirements. Score from 0-10 on:\n",
        "        1. Skills Match\n",
        "        2. Experience Level\n",
        "        3. Overall Fit\n",
        "\n",
        "        Provide detailed feedback and clear recommendation.\"\"\"),\n",
        "        HumanMessage(content=f\"\"\"\n",
        "        Job Requirements:\n",
        "        {state.job_requirements.model_dump_json()}\n",
        "\n",
        "        CV Content:\n",
        "        {state.candidates[-1].cv_content}\n",
        "        \"\"\")\n",
        "    ]\n",
        "\n",
        "    analysis = llm.invoke(messages)\n",
        "    score = float(re.search(r\"Overall Score:\\s*(\\d+\\.?\\d*)\", analysis.content).group(1))\n",
        "\n",
        "    tool_node = ToolNode([send_linkedin_message])\n",
        "\n",
        "    if score >= 7.0:\n",
        "        status = \"approved\"\n",
        "        message = f\"Congratulations! You've been selected for an interview on {state.job_requirements.interview_date}\"\n",
        "    else:\n",
        "        status = \"rejected\"\n",
        "        message = \"Thank you for your application. Unfortunately...\"\n",
        "\n",
        "    # Send response via LinkedIn\n",
        "    tool_node.invoke({\n",
        "        \"name\": \"send_linkedin_message\",\n",
        "        \"args\": {\n",
        "            \"profile_url\": state.candidates[-1].linkedin_url,\n",
        "            \"message\": message\n",
        "        }\n",
        "    })\n",
        "\n",
        "    return {\n",
        "        \"cv_score\": score,\n",
        "        \"status\": status,\n",
        "        \"feedback\": analysis.content,\n",
        "        \"phase\": \"prepare_interview\" if status == \"approved\" else \"complete\"\n",
        "    }\n",
        "\n",
        "\n",
        "# Interview preparation node\n",
        "def prepare_interview(state: RecruitmentState) -> Dict[str, Any]:\n",
        "    \"\"\"Generate interview questions with human approval\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"\"\"Generate 10 interview questions covering:\n",
        "        - Technical Skills (4)\n",
        "        - Experience (3)\n",
        "        - Problem Solving (3)\"\"\"),\n",
        "        HumanMessage(content=f\"\"\"\n",
        "        Position: {state.job_requirements.title}\n",
        "        Required Skills: {', '.join(state.job_requirements.skills_required)}\n",
        "        \"\"\")\n",
        "    ]\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    # Raise NodeInterrupt for human review\n",
        "    raise NodeInterrupt(\n",
        "        f\"Please review the interview questions:\\n\\n{response.content}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjLDE9WhtXcx"
      },
      "source": [
        "# Creating the recruitment workflow\n",
        "Combines workflow nodes into the overall recruitment process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r8tMD9d8ZdAg"
      },
      "outputs": [],
      "source": [
        "def create_recruitment_workflow():\n",
        "    \"\"\"Create the recruitment workflow graph\"\"\"\n",
        "    workflow = StateGraph(RecruitmentState)\n",
        "    memory = MemorySaver()\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"requirements_gathering\", requirements_gathering)\n",
        "    workflow.add_node(\"generate_job_desc\", generate_job_desc)\n",
        "    workflow.add_node(\"linkedin_process\", linkedin_process)\n",
        "    workflow.add_node(\"analyze_cv\", analyze_cv)\n",
        "    workflow.add_node(\"prepare_interview\", prepare_interview)\n",
        "\n",
        "    # Add edges with human-in-the-loop cycles\n",
        "    workflow.add_edge(START, \"requirements_gathering\")\n",
        "    workflow.add_edge(\"requirements_gathering\", \"generate_job_desc\")\n",
        "\n",
        "    def route_after_job_desc(state: RecruitmentState):\n",
        "        return \"linkedin_process\" if state.job_description_approved else \"generate_job_desc\"\n",
        "\n",
        "    def route_after_cv(state: RecruitmentState):\n",
        "        return \"prepare_interview\" if state.status == \"approved\" else END\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"generate_job_desc\",\n",
        "        route_after_job_desc,\n",
        "        [\"linkedin_process\", \"generate_job_desc\"]\n",
        "    )\n",
        "\n",
        "    workflow.add_edge(\"linkedin_process\", \"analyze_cv\")\n",
        "    workflow.add_conditional_edges(\n",
        "        \"analyze_cv\",\n",
        "        route_after_cv,\n",
        "        [\"prepare_interview\", END]\n",
        "    )\n",
        "    workflow.add_edge(\"prepare_interview\", END)\n",
        "\n",
        "    # Compile with breakpoints\n",
        "    graph = workflow.compile(\n",
        "        checkpointer=memory,\n",
        "        interrupt_before=[\"generate_job_desc\", \"prepare_interview\"]\n",
        "    )\n",
        "\n",
        "    # Generate and display the Mermaid visualization\n",
        "    print(\"\"\"graph TD\n",
        "        Start --> requirements_gathering\n",
        "        requirements_gathering --> generate_job_desc\n",
        "        generate_job_desc -->|Approved| linkedin_process\n",
        "        generate_job_desc -->|Not Approved| generate_job_desc\n",
        "        linkedin_process --> analyze_cv\n",
        "        analyze_cv -->|CV-Good| prepare_interview\n",
        "        analyze_cv -->|CV-Bad| End\n",
        "        prepare_interview -->|Approved| End\n",
        "        prepare_interview -->|Not Approved| prepare_interview\n",
        "    \"\"\")\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Job Requirements\n",
        "Prompts user for job details and returns a JobRequirements object."
      ],
      "metadata": {
        "id": "ocVanzGpamE7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3HclsfHXg89P"
      },
      "outputs": [],
      "source": [
        "def get_job_requirements():\n",
        "    job_title = input(\"What is the job title? \")\n",
        "    company_description = input(\"Provide a description of the company: \")\n",
        "    job_requirements = input(\"List the job requirements (comma-separated): \").split(\",\")\n",
        "    candidate_responsibilities = input(\"List the candidate responsibilities (comma-separated): \").split(\",\")\n",
        "    candidate_qualifications = input(\"List the candidate qualifications (comma-separated): \").split(\",\")\n",
        "    company_benefits = input(\"List the company benefits (comma-separated): \").split(\",\")\n",
        "    interview_date = input(\"What is the interview date (YYYY-MM-DD)? \")\n",
        "    preferred_country = input(\"What is the preferred country for the role? \")\n",
        "    years_experience = int(input(\"What is the required years of experience? \"))\n",
        "    linkedin_profiles = input(\"Provide any LinkedIn profile URLs (comma-separated, optional): \").split(\",\")\n",
        "    skills_required = input(\"List the required skills (comma-separated): \").split(\",\")\n",
        "    salary_range = input(\"What is the salary range for the role? \")\n",
        "\n",
        "    return JobRequirements(\n",
        "        title=job_title,\n",
        "        company_description=company_description,\n",
        "        job_requirements=job_requirements,\n",
        "        candidate_responsibilities = candidate_responsibilities,\n",
        "        candidate_qualifications = candidate_qualifications,\n",
        "        company_benefits = company_benefits,\n",
        "        interview_date = interview_date,\n",
        "        preferred_country = preferred_country,\n",
        "        years_experience = years_experience,\n",
        "        linkedin_profiles = linkedin_profiles,\n",
        "        skills_required = skills_required,\n",
        "        salary_range = salary_range,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the recruitment workflow instance\n",
        "recruitment_workflow = create_recruitment_workflow()\n",
        "\n",
        "# Set the test configuration\n",
        "config = {\"configurable\": {\"thread_id\": \"test_1\"}}\n",
        "\n",
        "# Define the initial state of the recruitment process\n",
        "initial_state = {\n",
        "    \"phase\": \"requirements_gathering\",\n",
        "    \"messages\": [],\n",
        "    \"job_requirements\": None,\n",
        "    \"job_description\": None,\n",
        "    \"job_description_approved\": False,\n",
        "    \"candidates\": [],\n",
        "    \"linkedin_process_complete\": False,\n",
        "    \"cv_analysis_complete\": False,\n",
        "    \"interview_questions\": None,\n",
        "    \"interview_questions_approved\": False\n",
        "}\n",
        "\n",
        "# Run the workflow and handle any interrupts\n",
        "try:\n",
        "    for event in recruitment_workflow.stream(initial_state, config, stream_mode=\"values\"):\n",
        "        print(\"\\nNew State Update:\")\n",
        "        print(f\"Phase: {event.get('phase')}\")\n",
        "        if event.get('messages'):\n",
        "            print(f\"Latest Message: {event['messages'][-1].content}\")\n",
        "except NodeInterrupt as e:\n",
        "    print(f\"\\nHuman Review Required: {str(e)}\")\n",
        "\n",
        "    # Example of handling job description approval\n",
        "    recruitment_workflow.update_state(\n",
        "        config,\n",
        "        {\"job_description_approved\": True}\n",
        "    )\n",
        "\n",
        "    # Continue the workflow execution\n",
        "    for event in recruitment_workflow.stream(None, config, stream_mode=\"values\"):\n",
        "        print(\"\\nNew State Update:\")\n",
        "        print(f\"Phase: {event.get('phase')}\")\n",
        "        if event.get('messages'):\n",
        "            print(f\"Latest Message: {event['messages'][-1].content}\")"
      ],
      "metadata": {
        "id": "d6rGpKV4d8ue"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}