{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Lvb9jrpDE52l8ZZWXeBoxbjXR5eHc6ft?usp=sharing)"
      ],
      "metadata": {
        "id": "Scl9jhVIu8Qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Portkey Agent Integration Cookbook\n",
        "\n",
        "## What is [Portkey AI](https://portkey.ai)?\n",
        "\n",
        "\n",
        "Portkey AI is a production infrastructure layer that makes your AI agents production-ready with just a few lines of code changes. It sits between your agents and LLM providers, adding powerful capabilities like routing, monitoring, and reliability features.\n",
        "\n",
        "\n",
        "Portkey adds 4 core production capabilities to any agent:\n",
        "1. Routing to 200+ LLMs\n",
        "2. Making each LLM call more robust\n",
        "3. Full-stack tracing & cost, performance analytics\n",
        "4. Real-time guardrails to enforce behavior\n",
        "\n",
        "\n",
        "# Framework Integrations\n",
        "\n",
        "E2E examples of major Agent Frameworks:\n",
        "1. Autogen\n",
        "2. CrewAI\n",
        "4. Langchain Agents\n",
        "5. LangGraph Agents\n",
        "6. OpenAI Swarm and more\n",
        "\n",
        "## Key Features\n",
        "Portkey offers a range of advanced features to enhance your agents. Here‚Äôs an overview\n",
        "\n",
        "| Feature | Description |\n",
        "|---------|-------------|\n",
        "| üåê  Multi-LLM Integration | Access 200+ LLMs with simple configuration changes |\n",
        "| üõ°Ô∏è Enhanced Reliability | Implement fallbacks, load balancing, retries, and much more |\n",
        "| üìä Advanced Metrics | Track costs, tokens, latency, and 40+ custom metrics effortlessly |\n",
        "| üîç Detailed Traces and Logs | Gain insights into every agent action and decision |\n",
        "| üöß Guardrails Enforce agent behavior with real-time checks on inputs and outputs |\n",
        "| üîÑ Continuous Optimization | Capture user feedback for ongoing agent improvements |\n",
        "| üíæ Smart Caching | Reduce costs and latency with built-in caching mechanisms |\n",
        "| üîê Enterprise-Grade Security | Set budget limits and implement fine-grained access controls |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u1sOzdp4hCKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here‚Äôs the output from your Agent‚Äôs run on Portkey's dashboard\n",
        "\n",
        "<img src=\"https://github.com/siddharthsambharia-portkey/Portkey-Product-Images/blob/main/Portkey-Dashboard.png?raw=true\" alt=\"Portkey Dashboard\" />\n"
      ],
      "metadata": {
        "id": "lI0MzSjEkTaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting:\n",
        "- Get your Portkey API key from [Portkey Dashboard](https://app.portkey.ai)\n",
        "- Install the Portkey SDK: `pip install portkey-ai`\n",
        "- Have your LLM provider API keys ready\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WHugyLuRhS5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Autogen**\n",
        "\n",
        "In this example, we're creating a development team of agents to build a game:\n",
        "\n",
        "A Product Manager to define requirements\n",
        "A Coder to implement the game\n",
        "A User Proxy to facilitate interaction"
      ],
      "metadata": {
        "id": "tAYhSyZ7Ab_h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hoZeh__AJcI"
      },
      "outputs": [],
      "source": [
        "!pip install -qU pyautogen portkey-ai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
        "from google.colab import userdata\n",
        "\n",
        "# Import the portkey library to fetch helper functions\n",
        "from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders\n",
        "\n",
        "gpt3_config = [\n",
        "    {\n",
        "        \"api_key\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "        \"model\": \"gpt-3.5-turbo\",\n",
        "        \"base_url\": PORTKEY_GATEWAY_URL,\n",
        "        \"api_type\": \"openai\",\n",
        "        \"default_headers\": createHeaders(\n",
        "            api_key = userdata.get(\"PORTKEY_API_KEY\"),\n",
        "            provider = \"openai\",\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "gpt4o_config = [\n",
        "    {\n",
        "        \"api_key\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "        \"model\": \"gpt-4o\",\n",
        "        \"base_url\": PORTKEY_GATEWAY_URL,\n",
        "        \"api_type\": \"openai\",\n",
        "        \"default_headers\": createHeaders(\n",
        "            api_key = userdata.get(\"PORTKEY_API_KEY\"),\n",
        "            provider = \"openai\",\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "llama3_config = [\n",
        "    {\n",
        "        \"api_key\": userdata.get(\"GROQ_API_KEY\"),\n",
        "        \"model\": \"llama3-70b-8192\",\n",
        "        \"base_url\": PORTKEY_GATEWAY_URL,\n",
        "        \"api_type\": \"openai\", # Portkey conforms to the openai api_type\n",
        "        \"default_headers\": createHeaders(\n",
        "            api_key = userdata.get(\"PORTKEY_API_KEY\"),\n",
        "            provider = \"groq\",\n",
        "        )\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "OpIc2sqEAll1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import autogen\n",
        "\n",
        "# Create user proxy agent, coder, product manager\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"User_proxy\",\n",
        "    system_message=\"A human admin who will give the idea and run the code provided by Coder.\",\n",
        "    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
        "    human_input_mode=\"ALWAYS\",\n",
        ")\n",
        "coder = autogen.AssistantAgent(\n",
        "    name=\"Coder\",\n",
        "    system_message = \"You are talented Python developer who is good at developing games. You closely work with Product Manager.\"\n",
        "    llm_config={\"config_list\": llama3_config},\n",
        ")\n",
        "pm = autogen.AssistantAgent(\n",
        "    name=\"product_manager\",\n",
        "    system_message=\"You will help break down the initial idea into a well scoped requirement for the coder; Do not involve in future conversations or error fixing\",\n",
        "    llm_config={\"config_list\": gpt3_config},\n",
        ")\n",
        "\n",
        "# Create groupchat\n",
        "groupchat = autogen.GroupChat(\n",
        "    agents=[user_proxy, coder, pm], messages=[])\n",
        "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config={\"config_list\": gpt4o_config})\n",
        "\n",
        "# Start the conversation\n",
        "user_proxy.initiate_chat(\n",
        "    manager, message=\"Build a classic & basic pong game with 2 players in python\")"
      ],
      "metadata": {
        "id": "RF-kZdyGAe-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CrewAI**\n",
        "\n",
        "Here we're building a research and analysis team that works together to analyze a dataset:\n",
        "\n",
        "A Data Analyst to crunch numbers\n",
        "A Research Writer to create reports\n",
        "A Reviewer to validate findings"
      ],
      "metadata": {
        "id": "NRcGApabAqVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU crewai portkey-ai"
      ],
      "metadata": {
        "id": "Z3tbZGGVA4dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL\n",
        "from google.colab import userdata\n",
        "import getpass\n",
        "\n",
        "portkey_api_key = getpass.getpass(\"Enter your Portkey API Key: \")\n",
        "openai_key = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "deepinfra_key = getpass.getpass(\"Enter your Deepinfra API Key: \")\n",
        "\n",
        "gpt_3 = ChatOpenAI(\n",
        "    api_key=openai_key,\n",
        "    base_url=PORTKEY_GATEWAY_URL,\n",
        "    default_headers=createHeaders(api_key=portkey_api_key,provider=\"openai\")\n",
        ")\n",
        "\n",
        "llama3 = ChatOpenAI(\n",
        "    api_key = deepinfra_key,\n",
        "    base_url=PORTKEY_GATEWAY_URL,\n",
        "    model = \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
        "    default_headers=createHeaders(api_key=portkey_api_key,provider=\"deepinfra\")\n",
        ")"
      ],
      "metadata": {
        "id": "8fNj-4LnAp-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew, Process\n",
        "\n",
        "# Define your agents with roles and goals\n",
        "product_manager = Agent(\n",
        "    role='Product Manager',\n",
        "    goal='Define requirements for a software product',\n",
        "    backstory=\"You are an experienced Product Manager skilled in defining clear and concise requirements.\",\n",
        "    llm = gpt_3\n",
        ")\n",
        "\n",
        "coder = Agent(\n",
        "    role='Software Developer',\n",
        "    goal='Develop software based on the provided requirements',\n",
        "    backstory=\"You are a skilled software developer proficient in coding robust and efficient applications.\",\n",
        "    llm = llama3\n",
        ")\n",
        "\n",
        "# Create tasks for your agents\n",
        "task1 = Task(\n",
        "    description=\"Define the key requirements and features for a classic ping pong game. Be specific and concise.\",\n",
        "    expected_output=\"A clear and concise list of requirements for the ping pong game\",\n",
        "    agent=product_manager\n",
        ")\n",
        "\n",
        "task2 = Task(\n",
        "    description=\"Based on the provided requirements, develop the code for the classic ping pong game. Focus on gameplay mechanics and a simple user interface.\",\n",
        "    expected_output=\"Complete code for the ping pong game\",\n",
        "    agent=coder\n",
        ")\n",
        "\n",
        "# Instantiate your crew with a sequential process\n",
        "crew = Crew(\n",
        "    agents=[product_manager, coder],\n",
        "    tasks=[task1, task2],\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Get your crew to work!\n",
        "result = crew.kickoff()\n",
        "print(\"######################\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "syHhFJcdA6Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Langchain Agents**"
      ],
      "metadata": {
        "id": "X7_GtUOHA_9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_openai portkey-ai langchain langchain_community duckduckgo-search langchainhub"
      ],
      "metadata": {
        "id": "4ffPvtQYqurF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "from langchain_openai import ChatOpenAI\n",
        "from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL, Portke\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from portkey_ai import Portkey\n",
        "\n",
        "# Initialize the DuckDuckGo search tool\n",
        "search_tool = DuckDuckGoSearchResults(max_results=5)\n",
        "\n",
        "# Ensure tools is a list containing the search tool\n",
        "tools = [search_tool]\n",
        "\n",
        "\n",
        "client = Portkey(\n",
        "    api_key=\"PORTKEY_API_KEY\",  #Enter Your PORTKEY API KEY\n",
        ")\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/react-chat\")\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    api_key=\"OPENAI_API_KEY\", #YOUR OPENAI API KEY\n",
        "    base_url=PORTKEY_GATEWAY_URL,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    default_headers=createHeaders(api_key=\"PORTKEY_API_KEY\", provider=\"openai\") #YOUR PORTKEY API KEY\n",
        ")\n",
        "\n",
        "# Construct the ReAct agent\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "\n",
        "# Create an agent executor by passing in the agent and tools\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "8hdH2A7xBCUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Langraph Agents**\n",
        "\n",
        "In this example, we're building a conversational agent with state management:\n",
        "- Maintains conversation context\n",
        "- Handles multi-turn dialogues\n",
        "- Provides streaming responses\n",
        "- Demonstrates graph-based agent architecture\n",
        "\n",
        "Ideal for:\n",
        "- Customer service chatbots\n",
        "- Interactive assistants\n",
        "- Multi-step conversations\n"
      ],
      "metadata": {
        "id": "eGIzfMN9A65-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langgraph langchain_openai portkey-ai\n"
      ],
      "metadata": {
        "id": "oUylyW_0Banc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from langchain_openai import ChatOpenAI\n",
        "from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    api_key=\"OpenAI_API_Key\",\n",
        "    base_url=PORTKEY_GATEWAY_URL,\n",
        "    default_headers=createHeaders(\n",
        "        provider=\"openai\",\n",
        "        api_key=\"PORTKEY_API_KEY\",\n",
        "    )\n",
        ")\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph_builder.set_finish_point(\"chatbot\")\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break\n"
      ],
      "metadata": {
        "id": "O5_oyQ54BYrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LlamaIndex agents**"
      ],
      "metadata": {
        "id": "2EY_nHiABc5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU llama-agents llama-index portkey-ai\n"
      ],
      "metadata": {
        "id": "L8iwEplKBcob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders\n",
        "\n",
        "gpt_4o_config = {\n",
        "    \"provider\": \"openai\", #Use the provider of choice\n",
        "    \"api_key\": \"YOUR_OPENAI_KEY,\n",
        "    \"override_params\": { \"model\":\"gpt-4o\" }\n",
        "}\n",
        "\n",
        "gpt_4o = OpenAI(\n",
        "    api_base=PORTKEY_GATEWAY_URL,\n",
        "    default_headers=createHeaders(\n",
        "        api_key=userdata.get('PORTKEY_API_KEY'),\n",
        "        config=gpt_4o_config\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "zNeyw7NBBgU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "gpt_4o_config = {\n",
        "    \"provider\": \"openai\",\n",
        "    \"api_key\": userdata.get('OPENAI_API_KEY'),\n",
        "    \"override_params\": { \"model\":\"gpt-4o\" }\n",
        "}\n",
        "\n",
        "gpt_4o = OpenAI(\n",
        "    api_base=PORTKEY_GATEWAY_URL,\n",
        "    default_headers=createHeaders(\n",
        "        api_key=userdata.get('PORTKEY_API_KEY'),\n",
        "        config=gpt_4o_config\n",
        "    )\n",
        ")\n",
        "\n",
        "llama3_groq_config = {\n",
        "    \"provider\": \"groq\",\n",
        "    \"api_key\": userdata.get('GROQ_API_KEY'),\n",
        "    \"override_params\": {\"model\":\"llama3-70b-8192\"}\n",
        "}\n",
        "\n",
        "llama3 = OpenAI(\n",
        "    api_base=PORTKEY_GATEWAY_URL,\n",
        "    default_headers=createHeaders(\n",
        "        api_key=userdata.get('PORTKEY_API_KEY'),\n",
        "        config=llama3_groq_config\n",
        "    )\n",
        ")\n",
        "\n",
        "# CREATING THE AGENT\n",
        "from llama_agents import (\n",
        "    AgentService,\n",
        "    AgentOrchestrator,\n",
        "    ControlPlaneServer,\n",
        "    SimpleMessageQueue,\n",
        ")\n",
        "\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "\n",
        "# create an agent\n",
        "def get_the_secret_fact() -> str:\n",
        "    \"\"\"Returns the secret fact.\"\"\"\n",
        "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
        "\n",
        "\n",
        "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
        "\n",
        "agent1 = ReActAgent.from_tools([tool], llm=llama3)\n",
        "agent2 = ReActAgent.from_tools([], llm=llama3)\n",
        "\n",
        "# create our multi-agent framework components\n",
        "message_queue = SimpleMessageQueue()\n",
        "control_plane = ControlPlaneServer(\n",
        "    message_queue=message_queue,\n",
        "    orchestrator=AgentOrchestrator(llm=gpt_4o),\n",
        ")\n",
        "\n",
        "agent_server_1 = AgentService(\n",
        "    agent=agent1,\n",
        "    message_queue=message_queue,\n",
        "    description=\"Useful for getting the secret fact.\",\n",
        "    service_name=\"secret_fact_agent\"\n",
        ")\n",
        "\n",
        "agent_server_2 = AgentService(\n",
        "    agent=agent2,\n",
        "    message_queue=message_queue,\n",
        "    description=\"Useful for getting random dumb facts.\",\n",
        "    service_name=\"dumb_fact_agent\"\n",
        ")"
      ],
      "metadata": {
        "id": "ncis36edBoCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_agents import LocalLauncher\n",
        "\n",
        "# launch it\n",
        "launcher = LocalLauncher(\n",
        "    [agent_server_1, agent_server_2],\n",
        "    control_plane,\n",
        "    message_queue,\n",
        ")\n",
        "result = launcher.launch_single(\"What is the secret fact?\")\n",
        "\n",
        "print(f\"Result: {result}\")"
      ],
      "metadata": {
        "id": "vfXZhhM2BrVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OpenAI Swarm**\n",
        "\n",
        "Here's a simple weather agent built using OpenAI Swarm"
      ],
      "metadata": {
        "id": "TnWZfczuBYYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from swarm import Swarm, Agent\n",
        "from portkey_ai import Portkey\n",
        "\n",
        "portkey = Portkey(\n",
        "    api_key=userdata.get('PORTKEY_API_KEY'), # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "    Authorization=\"YOUR_OPENAI_API_KEY\",\n",
        "    provider=\"openai\n",
        "    )\n",
        "\n",
        "client = Swarm(client=portkey)\n",
        "\n",
        "\n",
        "def get_weather(location) -> str:\n",
        "    return \"{'temp':67, 'unit':'F'}\"\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Agent\",\n",
        "    instructions=\"You are a helpful agent.\",\n",
        "    functions=[get_weather],\n",
        ")\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"What's the weather in NYC?\"}]\n",
        "\n",
        "response = client.run(agent=agent, messages=messages)\n",
        "print(response.messages[-1][\"content\"])"
      ],
      "metadata": {
        "id": "l4dy7O27Bzyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Portkey Features\n",
        "\n",
        "### Interoperability\n",
        "\n",
        "Easily switch between **200+ LLMs** by changing the configuration in your client,. In this example we are using Virtual Keys to sseamlessly change our LLM client. Portkey offers a virtual key system that allows you to securely store your Language Model (LLM) API keys in their vault. This system uses a unique virtual identifier to simplify API key management.\n",
        "\n",
        "\n",
        "#### Example: Switching from OpenAI to Azure OpenAI\n",
        "\n",
        "```python\n",
        "portkey = Portkey(\n",
        "    api_key=userdata.get('PORTKEY_API_KEY'), # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "    virtual_key=\"YOUR_OPENAI_VIRTUAL_KEY\",\n",
        "    )\n",
        "\n",
        "# Changing the virtual-key to shift to shift to Azure\n",
        "\n",
        "portkey = Portkey(\n",
        "    api_key=userdata.get('PORTKEY_API_KEY'), # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "    virtual_key=\"YOUR_AZURE_VIRTUAL_KEY\",\n",
        "    )\n",
        "```\n",
        "\n",
        "\n",
        "### Reliability\n",
        "\n",
        "Implement fallbacks, load balancing, and automatic retries to make your agents more resilient.\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"strategy\": {\n",
        "    \"mode\": \"fallback\" # Options: \"loadbalance\" or \"fallback\"\n",
        "  },\n",
        "  \"targets\": [\n",
        "    {\n",
        "      \"provider\": \"openai\",\n",
        "      \"api_key\": \"openai-api-key\",\n",
        "      \"override_params\": {\n",
        "        \"top_k\": \"0.4\",\n",
        "        \"max_tokens\": \"100\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"provider\": \"anthropic\",\n",
        "      \"api_key\": \"anthropic-api-key\",\n",
        "      \"override_params\": {\n",
        "        \"top_p\": \"0.6\",\n",
        "        \"model\": \"claude-3-5-sonnet-20240620\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "Learn more about [Portkey Config object here](https://docs.portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/configs).\n",
        "Be Careful to Load-Balance/Fallback to providers that don't support tool calling when the request contains a function call.\n",
        "### Metrics\n",
        "\n",
        "Agent runs are complex. Portkey automatically logs **40+ comprehensive metrics** for your AI agents, including cost, tokens used, latency, etc. Whether you need a broad overview or granular insights into your agent runs, Portkey's customizable filters provide the metrics you need.\n",
        "\n",
        "<details>\n",
        "  <summary><b>Portkey's Observability Dashboard</b></summary>\n",
        "<img src=https://github.com/siddharthsambharia-portkey/Portkey-Product-Images/blob/main/Portkey-Dashboard.png?raw=true width=70%\" alt=\"Portkey Dashboard\" />\n",
        "</details>\n",
        "\n",
        "### Comprehensive Logging\n",
        "\n",
        "Access detailed logs and traces of agent activities, function calls, and errors. Filter logs based on multiple parameters for in-depth analysis.\n",
        "\n",
        "<details>\n",
        "  <summary><b>Traces</b></summary>\n",
        "  <img src=\"https://raw.githubusercontent.com/siddharthsambharia-portkey/Portkey-Product-Images/main/Portkey-Traces.png\" alt=\"Portkey Logging Interface\" width=70% />\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "  <summary><b>Logs</b></summary>\n",
        "  <img src=\"https://raw.githubusercontent.com/siddharthsambharia-portkey/Portkey-Product-Images/main/Portkey-Logs.png\" alt=\"Portkey Metrics Visualization\" width=70% />\n",
        "</details>\n",
        "\n",
        "### Guardrails\n",
        "AI agents, while powerful, can sometimes produce unexpected or undesired outputs. Portkey's Guardrails feature helps enforce agent behavior in real-time, ensuring your agents operate within specified parameters. Verify both the **inputs** to and *outputs* from your agents to ensure they adhere to specified formats and content guidelines. Learn more about Portkey's Guardrails [here](https://docs.portkey.ai/product/guardrails)\n",
        "\n",
        "### Continuous Improvement\n",
        "\n",
        "Capture qualitative and quantitative user feedback on your requests to continuously enhance your agent performance.\n",
        "\n",
        "### Caching\n",
        "\n",
        "Reduce costs and latency with Portkey's built-in caching system.\n",
        "\n",
        "```python\n",
        "portkey_config = {\n",
        " \"cache\": {\n",
        "    \"mode\": \"semantic\"  # Options: \"simple\" or \"semantic\"\n",
        " }\n",
        "}\n",
        "```\n",
        "\n",
        "### Security and Compliance\n",
        "\n",
        "Set budget limits on provider API keys and implement fine-grained user roles and permissions for both your application and the Portkey APIs.\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "- [üìò Portkey Documentation](http://docs.portkey.ai/?utm_source=NirDiamant&utm_medium=github&utm_campaign=external-integration)\n",
        "- [üê¶ Twitter](https://twitter.com/portkeyai/)\n",
        "- [üí¨ Discord Community](https://discord.gg/JHPt4C7r)\n",
        "- [üìä Portkey App](https://app.portkey.ai/?utm_source=NirDiamant&utm_medium=github&utm_campaign=external-integration)\n",
        "\n",
        "For more information on using these features and setting up your Config, please refer to the [Portkey documentation](https://docs.portkey.ai/?utm_source=NirDiamant&utm_medium=github&utm_campaign=external-integration)."
      ],
      "metadata": {
        "id": "X2RyaPZzhg9t"
      }
    }
  ]
}
